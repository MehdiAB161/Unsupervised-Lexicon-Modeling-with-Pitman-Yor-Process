{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk import ngrams\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Restaurants(object) :\n",
    "    def __init__(self, global_dish_list):\n",
    "        self.restaurants_dict = {}\n",
    "        self.global_dish_list = global_dish_list\n",
    "        \n",
    "    def add_restaurant(self, context, restaurant) :\n",
    "        self.restaurants_dict[context] = restaurant\n",
    "    \n",
    "    def draw_new_customer(self, context, d, theta) :\n",
    "        self.restaurants_dict[context].draw_new_customer(self, d, theta)\n",
    "        \n",
    "    def dish_probability(self, context, d, theta) :\n",
    "        self.restaurants_dict[context].dish_probability(self, d, theta)\n",
    "        \n",
    "class Restaurant(object) :\n",
    "    def __init__(self, customers_per_table , dishes, context):\n",
    "        self.nb_tables = len(customers_per_table)\n",
    "        self.nb_dishes = len(set(dishes))\n",
    "        self.context = context\n",
    "        \n",
    "        self.customers_per_table = customers_per_table\n",
    "        self.dishes = dishes\n",
    "        self.dishes_counts = occurences_counts(dishes)\n",
    "        \n",
    "    def draw_new_customer(self, d, theta):\n",
    "        # TODO increment the counts\n",
    "        if len(context) == 0 :\n",
    "            table = draw_table(self.customers_per_table, self.dishes_counts, d, theta)\n",
    "            # If the table is a new table\n",
    "            if table == nb_tables : \n",
    "                customer = np.random.choice(super.dish_list)\n",
    "            else :\n",
    "                # TODO : Check the chosen word\n",
    "                customer = self.dishes[table]\n",
    "\n",
    "        else :\n",
    "            table = draw_table(self.customers_per_table, self.dishes_counts, d, theta)\n",
    "\n",
    "            # If the table is a new table\n",
    "            if table == nb_tables : \n",
    "                sub_context = self.context\n",
    "                del sub_context[0]\n",
    "                customer = super.draw_word(sub_context, d, theta)\n",
    "            else :\n",
    "                # TODO : Check the chosen word\n",
    "                customer = self.dishes[table]\n",
    "\n",
    "        return customer\n",
    "    \n",
    "    def dish_probability(self, d, theta):\n",
    "    # TODO increment the counts\n",
    "    if len(context) == 0 :\n",
    "        table = draw_table(self.customers_per_table, self.dishes_counts, d, theta)\n",
    "        # If the table is a new table\n",
    "        if table == nb_tables : \n",
    "            customer = np.random.choice(super.dish_list)\n",
    "        else :\n",
    "            # TODO : Check the chosen word\n",
    "            customer = self.dishes[table]\n",
    "\n",
    "    else :\n",
    "        table = draw_table(self.customers_per_table, self.dishes_counts, d, theta)\n",
    "\n",
    "        # If the table is a new table\n",
    "        if table == nb_tables : \n",
    "            sub_context = self.context\n",
    "            del sub_context[0]\n",
    "            customer = super.draw_word(sub_context, d, theta)\n",
    "        else :\n",
    "            # TODO : Check the chosen word\n",
    "            customer = self.dishes[table]\n",
    "\n",
    "    return customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_probability(word, context, n_gram_counters, d, theta, word_list):\n",
    "    \"\"\" Returns the probability that the word after the context will be w\"\"\"\n",
    "    \n",
    "    pitman_process_level = len(context)\n",
    "    if pitman_process_level > 0 :\n",
    "        context_word_counts = context_counts(context, n_gram_counters[pitman_process_level])\n",
    "    else :\n",
    "        context_word_counts = np.array([item for item in n_gram_counters[0].items()])\n",
    "    \n",
    "    nb_words = len(word_list)\n",
    "    nb_tables = context_word_counts.shape[0]\n",
    "        \n",
    "    if len(context) == 0 :\n",
    "        probability_distribution = compute_pyp_probabilities(context_word_counts, d, theta)\n",
    "    \n",
    "    else :\n",
    "        table = draw_table(context_word_counts, d, theta)\n",
    "        \n",
    "        # If the table is a new table\n",
    "        if table == nb_tables : \n",
    "            del context[0]\n",
    "            customer = draw_word(context, n_gram_counters, d, theta, word_list)\n",
    "        else :\n",
    "            # TODO : Check the chosen word\n",
    "            customer = context_word_counts[table, 0]\n",
    "\n",
    "    return customer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_gram_counter(file_data, n):\n",
    "    counts = Counter()\n",
    "    tokens = nltk.word_tokenize(file_data)\n",
    "    ngram = ngrams(tokens, n)\n",
    "    for n_gram in ngram :\n",
    "        counts[n_gram] += 1\n",
    "    return np.array([list(n_gram_count[0]) + [n_gram_count[1]]  for n_gram_count in list(counts.items())])\n",
    "\n",
    "def context_counts(context, n_gram_counter):\n",
    "    n = len(context)\n",
    "    return np.array([[item[0][n], item[1]] \n",
    "                     for item in n_gram_counter.items() if \"\".join(item[0][0:n]) == \"\".join(context)])\n",
    "\n",
    "def genrate_word_list(file_data) :\n",
    "    tokens = nltk.word_tokenize(file_data)\n",
    "    ngram = ngrams(tokens, n=1)\n",
    "    return [word[0] for word in ngram]\n",
    "\n",
    "def occurences_counts(input_list):\n",
    "    counter = Counter(input_list)\n",
    "    return [counter[input_list[i]] for i in range(len(input_list))]\n",
    "\n",
    "def compute_pyp_probabilities(customer_counts, dish_counts, d, theta) :\n",
    "    \"\"\" Compute the tables probabilities with respect to the PYP distribution\n",
    "    The last probability is the new table probability\"\"\"\n",
    "    # TODO : Adapt the algorithm so that more than one table can hold a word\n",
    "    # TODO Adapt the table count size\n",
    "    table_counts_per_type = list(np.ones(len(customer_counts)))\n",
    "\n",
    "    nb_tables = len(table_counts_per_word)\n",
    "    nb_customers = sum(customer_counts)\n",
    "\n",
    "    old_tables_probabilities = [(customer_counts[i] - d * dish_counts[i] ) / (nb_customers + theta)\n",
    "                                    for i in range(nb_tables)] \n",
    "    new_table_probability = [(theta + d * nb_tables) / (nb_customers + theta)]\n",
    "    all_probabilities = old_tables_probabilities + new_table_probability\n",
    "    \n",
    "    assert np.sum(all_probabilities) == 1\n",
    "    \n",
    "    return all_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_table(customer_list, d, theta):\n",
    "    \"\"\" Draws a table with respect to PYP \n",
    "    The last table is a new table\"\"\"\n",
    "    all_probabilities = compute_pyp_probabilities(customer_counts, dish_counts, d, theta)\n",
    "    \n",
    "    picked_table = np.argmax(np.random.multinomial(n=1, pvals=all_probabilities))\n",
    "    return picked_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_restaurants(train_data) :\n",
    "    \n",
    "    # TODO : convert to strnig of tuple\n",
    "    \n",
    "    # Fill the restaurants :\n",
    "    word_list = genrate_word_list(train_data)\n",
    "    restaurants = Restaurants(global_dish_list=word_list)\n",
    "\n",
    "    # Unigrams :\n",
    "    unigram_table =  n_gram_counter(train_data, n=1)\n",
    "    words = unigram_table[:, 0]\n",
    "    counts = unigram_table[:, 1].astype(int)\n",
    "    restaurants.add_restaurant(restaurant=Restaurant(customers_per_table=counts,\n",
    "                                          dishes=words, \n",
    "                                          context=\"\"),\n",
    "                              context=\"\")\n",
    "\n",
    "    # Bigrams : \n",
    "    bigram_table =  n_gram_counter(train_data, n=2)\n",
    "    nb_bigrams = bigram_table.shape[0] \n",
    "    words = bigram_table[:, 0:2]\n",
    "    counts = bigram_table[:, 2].astype(int)\n",
    "    unique_pregrams = np.array(list(set(words[:, 0])))\n",
    "\n",
    "    for pregram in unique_pre_grams :\n",
    "        to_keep = np.where(bigram_table[:, 0] == pregram)\n",
    "        sub_counts = bigram_table[to_keep, 1:3][0]\n",
    "        restaurants.add_restaurant(restaurant = Restaurant(customers_per_table=sub_counts[:,1],\n",
    "                                                          dishes=sub_counts[:,0].astype(int), \n",
    "                                                          context=pregram),\n",
    "                                   context=pregram) \n",
    "\n",
    "    # Trigrams :\n",
    "    trigram_table =  n_gram_counter(train_data, n=3)\n",
    "    nb_trigrams = trigram_table.shape[0] \n",
    "    words = trigram_table[:, 0:3]\n",
    "    counts = trigram_table[:, 3].astype(int)\n",
    "\n",
    "    pregrams = [str(item) for item in zip(words[:, 0], words[:, 1])]\n",
    "    unique_pre_grams = list(set(pregrams))\n",
    "    pregrams = np.array(pregrams)\n",
    "    for pregram in unique_pre_grams :\n",
    "        to_keep = np.where(pregrams == pregram)\n",
    "        sub_counts = trigram_table[to_keep, 2:4][0]\n",
    "        restaurants.add_restaurant(restaurant = Restaurant(customers_per_table=sub_counts[:,1],\n",
    "                                                          dishes=sub_counts[:,0].astype(int), \n",
    "                                                          context=pregram),\n",
    "                                   context=pregram)\n",
    "    return restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Restaurants' object has no attribute 'draw_new_customer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-46ed373684ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrestaurants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_new_customer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"he\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Restaurants' object has no attribute 'draw_new_customer'"
     ]
    }
   ],
   "source": [
    "train_file = \"Verne.5semaines.en\"\n",
    "\n",
    "with open(\"data/\" + train_file, \"r\") as text_file :\n",
    "    train_data = ''.join(text_file.read().split('\\n'))\n",
    "\n",
    "restaurants = fill_restaurants(train_data)\n",
    "restaurants.draw_new_customer(context=\"he\", d=.5, theta=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importance sampling\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
