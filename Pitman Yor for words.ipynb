{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk import ngrams\n",
    "\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_gram_counter(file_data, n):\n",
    "    counts = Counter()\n",
    "    tokens = nltk.word_tokenize(file_data)\n",
    "    ngram = ngrams(tokens, n)\n",
    "    for n_gram in ngram :\n",
    "        counts[n_gram] += 1\n",
    "    return counts\n",
    "\n",
    "def context_counts(context, n_gram_counter):\n",
    "    n = len(context)\n",
    "    return np.array([[item[0][n], item[1]] \n",
    "                     for item in n_gram_counter.items() if \"\".join(item[0][0:n]) == \"\".join(context)])\n",
    "\n",
    "def genrate_word_list(file_data) :\n",
    "    tokens = nltk.word_tokenize(file_data)\n",
    "    ngram = ngrams(tokens, n=1)\n",
    "    return [word[0] for word in ngram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"Verne.5semaines.en\"\n",
    "\n",
    "with open(\"data/\" + train_file, \"r\") as text_file :\n",
    "    train_data = ''.join(text_file.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_table(customer_list, d, theta):\n",
    "    customer_counts = list(map(int, customer_list[:,1]))\n",
    "    # TODO : Adapt the algorithm so that more than one table can hold a word\n",
    "    # TODO Adapt the table count size\n",
    "    table_counts_per_word = list(np.ones(len(customer_counts)))\n",
    "\n",
    "    nb_tables = len(table_counts_per_word)\n",
    "    nb_customers = sum(customer_counts)\n",
    "\n",
    "    old_tables_probabilities = [(customer_counts[i] - d *table_counts_per_word[i] ) / (nb_customers + theta)\n",
    "                                    for i in range(nb_tables)] \n",
    "    new_table_probability = [(theta + d * nb_tables) / (nb_customers + theta)]\n",
    "    all_probabilities = old_tables_probabilities + new_table_probability\n",
    "    \n",
    "    assert np.sum(all_probabilities) == 1\n",
    "    picked_table = np.argmax(np.random.multinomial(n=1, pvals=all_probabilities))\n",
    "    return picked_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_word(context, n_gram_counters, d, theta, word_list) :\n",
    "    pitman_process_level = len(context)\n",
    "    if pitman_process_level > 0 :\n",
    "        context_word_counts = context_counts(context, n_gram_counters[pitman_process_level])\n",
    "    else :\n",
    "        context_word_counts = np.array([item for item in n_gram_counters[0].items()])\n",
    "    \n",
    "    nb_words = len(word_list)\n",
    "    nb_tables = context_word_counts.shape[0]\n",
    "        \n",
    "    if len(context) == 0 :\n",
    "        table = draw_table(context_word_counts, d, theta)\n",
    "        \n",
    "        # If the table is a new table\n",
    "        if table == nb_tables : \n",
    "            customer = np.random.choice(word_list)\n",
    "        else :\n",
    "            # TODO : Check the chosen word\n",
    "            customer = context_word_counts[table, 0]\n",
    "    \n",
    "    else :\n",
    "        table = draw_table(context_word_counts, d, theta)\n",
    "        \n",
    "        # If the table is a new table\n",
    "        if table == nb_tables : \n",
    "            del context[0]\n",
    "            customer = draw_word(context, n_gram_counters, d, theta, word_list)\n",
    "        else :\n",
    "            # TODO : Check the chosen word\n",
    "            customer = context_word_counts[table, 0]\n",
    "\n",
    "    return customer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proved'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = [\"is\", \"the\"]\n",
    "d = 0.5\n",
    "theta = 1\n",
    "word_list = genrate_word_list(train_data)\n",
    "n_gram_counters = [n_gram_counter(train_data, n=n) for n in range(1, 4)]\n",
    "\n",
    "draw_word(context, n_gram_counters, d, theta, word_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
